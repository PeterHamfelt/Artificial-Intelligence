为什么要研究这些生成：



利用CG图像结合一些VR技术，可以让我们在有限的距离里遨游世界







根据方法划分



根据任务划分：

- pose 生成 pose generation
- 图像动起来 image animation
- 
- 



还有一些通用的基础知识：

- Distance Metric





一个大的分类其实是：

数据集只有2D的，生成3D的，3D 的和什么又是一致的呢？

多视角；新视角；

而建模整个3D模型，需要哪些信息呢？ -> camera viewpoint, object pose, object entities



可以被一个名词所涵盖：3D scene properties

we want to control: camera viewpoint, 3D pose, shape and appearance, multiple objects





## Single-View 3D Reconstruction

划分为：3D supervised| 2D supervised | unsupervised 



3D attributes, including camera, shape, texture, and light



重建什么？object's shape and texture

只需要一张图片吗？不，还需要2D image-level annotation

方法的发展进步：

- fit the parameters of a 3D prior morphable model (3DMM)

> building these prior models is expensive and time-consuming

- deep model 3D supervised reconstruction

> 需要3D ground truth, attributes or annotation

- 2D supervised reconstruction

> key modules is a differentiable render 





这里面又可以细分，category-specific 和 general 的





## novel view synthesis from 2D images

简称 NVS，他所要解决的问题是，从一些离散的观测中（不同视角的少量一些图片）



NVS techniques：可以先分成好几类

- explicitly reconstruction 

  只去构建表面

  然后这些方法无法生成高保真的结果

- volume-based representation 

相关工作有：Local Light Field Fusion， NeRF，Soft 3D Reconstruction for View Synthesis， SRN，Stereo Magnification: Learning View Synthesis using Multiplane Images

  model了整个空间，并且用volume rendering的方法生成图片

  这样带来的好处是：全局是连续的，都有了梯度，连续也可以高保真，

Neural Radiance Field (NeRF)



现存的一些方法是需要 每张图片的相机的参数，这个参数可以来自于

- 训练数据就有

- 通过一些技术估计 （例如 Structure-from-Motion）COLMAP

  



所以来了，怎么真正纯粹地只从RGB图像中生成呢？

- NeRF--



