## GAN

The important 2015 paper by Alec Radford, et al. titled “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks” introduced a stable model configuration for training deep convolutional neural network models as part of the GAN architecture.

In the paper, the authors explored the latent space for GANs fit on a number of different training datasets, most notably a dataset of celebrity faces. They demonstrated two interesting aspects.

The first was the vector arithmetic with faces. For example, a face of a smiling woman minus the face of a neutral woman plus the face of a neutral man resulted in the face of a smiling man.

```
smiling woman - neutral woman + neutral man = smiling man
```

Specifically, the arithmetic was performed on the points in the latent space for the resulting faces. Actually on the average of multiple faces with a given characteristic, to provide a more robust result.

The discriminator model takes as input one 80×80 color image an outputs a binary prediction as to whether the image is real (class=1) or fake (class=0). It is implemented as a modest convolutional neural network using best practices for GAN design such as using the LeakyReLU activation function with a slope of 0.2, using a 2×2 stride to downsample, and the adam version of stochastic gradient descent with a learning rate of 0.0002 and a momentum of 0.5

The generator model takes as input a point in the latent space and outputs a single 80×80 color image. This is achieved by using a fully connected layer to interpret the point in the latent space and provide sufficient activations that can be reshaped into many copies (in this case 128) of a low-resolution version of the output image (e.g. 5×5). This is then upsampled four times, doubling the size and quadrupling the area of the activations each time using transpose convolutional layers. The model uses best practices such as the LeakyReLU activation, a kernel size that is a factor of the stride size, and a hyperbolic tangent (tanh) activation function in the output layer.

### The development of GAN

- 模式坍塌（mode collapse）问题



生成器坍塌至对于不同的输入只能生成极其相似的样本 [29]；判别器损失迅速收敛至零 [179]，不能为生成器提供梯度更新；使生成器、判别器这一对模型难以收敛 [32]。



Existing GANs have suffered some training problems such as instability and mode collapse. Many related works have been done to fix these problems. I mainly followed several representative variants GANs below.

**DCGANs :** 

(abstract)

Supervised learning with convolutional networks (CNNs) has received much attention while unsupervised learning still has great research potential.  Deep convolutional generative adversarial networks (DCGANs) introduce unsupervised learning with CNNs to GANs, which greatly improves the stability of GANs training and the quality of the generated results. Additionally, the authors of DCGANs also show that the generated feature has interesting vector arithmetic properties which will help us understand and visualize what GANs learn, and the intermediate representations of multi-layer GANs.

(method)

They replaced all full connected layers by convolutional layers.





**WGAN :** 

WGAN replaced Jenson-Shannon divergence(JSD) by Wasserstein distance because JSD was not suitable to measure the distance between the generated data distribution and the real data distribution.

s in the paper used
r. $m,$ the batch size. r iteration. parameters. \begin{tabular}{l} 
Algorithm 1 WGAN, our proposed algorithm. All experiments i \\
the default values $\alpha=0.00005, c=0.01, m=64, n_{\text {critic }}=5 .$ \\
\hline Require: : $\alpha,$ the learning rate. $c,$ the clipping parameter. $m$ \\
$n_{\text {critic, the number of iterations of the critic per generator iter }}$ \\
Require: : $w_{0},$ initial critic parameters. $\theta_{0},$ initial generator's p \\
1: while $\theta$ has not converged do \\
2: for $t=0, \ldots, n_{\text {critic do }}$ \\
3: $\quad$ Sample $\left\{x^{(i)}\right\}_{i=1}^{m} \sim \mathbb{P}_{r}$ a batch from the real data. \\
4: $\quad$ Sample $\left\{z^{(i)}\right\}_{i=1}^{m} \sim p(z)$ a batch of prior samples. \\
5: $\quad g_{w} \leftarrow \nabla_{w}\left[\frac{1}{m} \sum_{i=1}^{m} f_{w}\left(x^{(i)}\right)-\frac{1}{m} \sum_{i=1}^{m} f_{w}\left(g_{\theta}\left(z^{(i)}\right)\right)\right]$ \\
6: $\quad w \leftarrow w+\alpha \cdot$ RMSProp $\left(w, g_{w}\right)$ \\
7: $\quad w \leftarrow \operatorname{clip}(w,-c, c)$ \\
8: end for \\
9: $\quad$ Sample $\left\{z^{(i)}\right\}_{i=1}^{m} \sim p(z)$ a batch of prior samples. \\
10: $\quad g_{\theta} \leftarrow-\nabla_{\theta} \frac{1}{m} \sum_{i=1}^{m} f_{w}\left(g_{\theta}\left(z^{(i)}\right)\right)$ \\
11: $\quad \theta \leftarrow \theta-\alpha \cdot \operatorname{RMSProp}\left(\theta, g_{\theta}\right)$ \\
12: end while
\end{tabular}





**WGAN-GP :** 

They find that WGAN using the clipping weights caused low quality samples and failure to converge. They used a new method of penalizing the norm of gradient of the critic with respect to its input.

**SinGAN**





EGANs this paper mainly evolves a population of generators to play the adversarial game with the discriminator. So they turn the process of confrontation training into an evolutionary problem. 



### Latent space

Decoupled 解耦

## Game AI

## Video Game level generation

