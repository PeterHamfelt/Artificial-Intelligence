# Few-Shot & Limited Data



## Introduction

On the one hand, we can utilize **transfer learning** with a pre-trained model.

On the other hand, we can 

Dynamic data-augmentation





## Literature

[Training Generative Adversarial Networks with Limited Data](https://arxiv.org/abs/2006.06676)  
**[`NeurIPS 2020`]** **(`NVIDIA`)** [[:octocat:](https://github.com/NVlabs/stylegan2-ada)] (*Tero Karras, Timo Aila*)

<details><summary>Click to expand</summary><p>


**Summary**

> Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge




</p></details>

---

Differentiable augmentation for data-efficient gan training



[Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis](https://arxiv.org/pdf/2101.04775.pdf)  
**[`ICLR 2021`]** **(`Rutgers`)** [[:octocat:](https://github.com/odegeasslbc/FastGAN-pytorch)] (*Bingchen Liu, Yizhe Zhu, Kunpeng Song, Ahmed Elgammal*)
<details><summary>Click to expand</summary><p>


**Summary**

> Use a skip-layer channel-wise excitation module and a self-supervised discriminator trained as a feature-encoder.

</p></details>

---