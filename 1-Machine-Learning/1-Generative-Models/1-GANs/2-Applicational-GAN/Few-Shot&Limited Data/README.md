# Few-Shot & Limited Data



## Introduction

On the one hand, we can utilize **transfer learning** with a pre-trained model.

On the other hand, we can 

Dynamic data-augmentation





## Literature

[Training Generative Adversarial Networks with Limited Data](https://arxiv.org/abs/2006.06676)

**[`NeurIPS 2020`]**	**(`NVIDIA`)**	**[`Tero Karras, Timo Aila`]**	**[[:octocat:](https://github.com/NVlabs/stylegan2-ada)]**

<details><summary>Click to expand</summary><p>


**Summary**

> Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge




</p></details>

---

Differentiable augmentation for data-efficient gan training



Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis  
**[`ICLR 2021`]** **(`Rutgers`)** *Bingchen Liu, Yizhe Zhu, Kunpeng Song, Ahmed Elgammal* [[:octocat:](https://github.com/odegeasslbc/FastGAN-pytorch)]

