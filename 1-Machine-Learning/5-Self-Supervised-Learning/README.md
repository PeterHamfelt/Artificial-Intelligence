# <p align=center>`Self-Supervised Learning` </p>

A collection of resources on Self-Supervised Learning.

## Introduction





### + Contrastive Learning

the hidden representations of two augmented views of the same object (positive pairs) are brought closer together, while those of different objects (negative pairs) are encouraged to be further apart.

Minimizing differences between positive pairs encourages modeling invariances, while contrasting negative pairs is thought to be required to prevent representational collapse (i.e., mapping all data to the same representation).



New 



[知乎-最简单的self-supervised方法](https://zhuanlan.zhihu.com/p/355523266)

![img](https://pic1.zhimg.com/80/v2-26d76f2cf52fa30a3646f3f6d0dbbea8_1440w.jpg)

## Literature

### Survey



### Category

<span id="Fastgan"></span>
[Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis](https://arxiv.org/pdf/2101.04775.pdf)  
**[`ICLR 2021`]  (`Rutgers`)**  
*Bingchen Liu, Yizhe Zhu, Kunpeng Song, Ahmed Elgammal*





[Barlow Twins: Self-Supervised Learning via Redundancy Reduction](https://arxiv.org/pdf/2103.03230.pdf)  
**[`ICML 2020`] (`Facebook, NYU`)**  
*Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, Stéphane Deny*

## Main Research Group

