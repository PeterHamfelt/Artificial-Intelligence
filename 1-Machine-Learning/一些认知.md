图像处理有三类最成功的算法

- 微分方程
- 压缩感知
- 稀疏表示





resnet 理解成求解 ODE



- 在Representation Learning 中，如何能等价的增大 Batch Size？ 如何能维持 Embedding Space 的稳定性？
- 在Deep Network 一定是最后一层具有最丰富的 Representation 吗？
- 听说Deep Network 的 Capacity 很强大 ，但时至今日，我们是否已经达到 Model 能负荷的上限？ （例如ResNet-50有 24M 个参数，号称拥有 '大数据' 的人们，是否已经触碰到 Effective Upper-Bound of ResNet-50's Model Complexity？）
- 如果Model Complexity 远超乎我们想象，那什么样的 Training Procedure 能最有效率的将信息储存于Deep Network 中？
- Data Augmentation是学习 Deep Learning 一定会接触到的方法，它只是一个方便 Training 的 Trick 呢？ 还是他对 Network 有特殊意义？
