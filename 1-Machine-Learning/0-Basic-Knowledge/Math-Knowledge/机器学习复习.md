## Bayes Classifier

1. Bayes Classification and Generative Models
2. Parameter Estimation
3. Bayesian Decision Rule

- **Training:**
  1. Collect training data from each class. 
  2. For each class $c$, estimate the class conditional densities $p(x|y=c)$:
  3. select a form of the distribution (e.g. Gaussian).
  4. estimate its parameters with MLE.
  5. Estimate the class priors $p(y)$ using MLE.
- **Classification:**
  1. Given a new sample $x^*$, calculate the likelihood $p(x^*|y=c)$ for each class $c$.
  2. Pick the class $c$ with largest posterior probability $p(y=c|x)$.
  
    - (equivalently, use $p(x|y=c)p(y=c)$ or $\log p(x|y=c)+\log p(y=c)$)

## Naive Bayes Classifier

1. Naive Bayes Gaussian Classifier - Iris dataset
2. Gaussian Classifier - Iris dataset
3. Naive Bayes Spam Classifier - Spam dataset

native used to deal with multiple features so assume each feature is independently

But actually, the really data seem to vary together, so the NB assumption isn’t accurate, we should use multivariate Gaussian



## Linear Classifiers

1. Discriminative linear classifiers
2. Logistic regression

## Support Vector Machines

1. Support vector machines (SVM)



## Non-Linear Classifiers

1. Nonlinear classifiers
2. Kernel trick and kernel SVM
3. Ensemble Methods - Boosting, Random Forests
4. Classification Summary





比较L1 L2 

L2 focuses more on large weights.

L1 treats all weights equally. The "corner" has some weights that are 0，L1 is applicable when there is a correlation between features.



cross-validation

Run many experiments on the training set to see which parameters work on different
versions of the data.





Why does SGD work?

SGD adds "noise" to the true gradient, the noise allows escaping/avoiding/jumping over small local minima.



## 监督学习：回归、分类

主流的监督学习算法

- 回归
  - 线性回归
- 分类
  - 朴素贝叶斯
  - 决策树
  - SCM
  - 逻辑回归

### Logistic regression

解决二分类问题，表示某件事情发生的可能性

用于离散

> 