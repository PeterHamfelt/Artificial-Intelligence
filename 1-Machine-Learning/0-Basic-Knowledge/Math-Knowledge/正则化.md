Regularized Least Square (RLS)



求解线性方程
$$
A x = b
$$
当解不存在或者不唯一时，就是病态问题 (ill-posed problem)。怎么来求解这样一个问题呢



对于解不存在的情况，可以增加一些条件找到一个近似解；

对于解不唯一的情况，可以增加一些限制缩小解的范围；



以上两种解决方法就时正则化方法，使得病态问题也能找到唯一解。


$$
A^T A x = A^T b
$$

$$
x = (A^T A)^{-1} A^T b
$$

if $\text{rank}(A) = n$
$$
(A^T A + \Gamma^T \Gamma) x = A^T b
$$

$$
x = (A^T A + \Gamma^T \Gamma)^{-1} A^T b
$$

if $\text{rank}(A^T A + \Gamma^T \Gamma) = n$



通常 $\Gamma = \alpha I$



## L0 

目的是希望权重尽可能多地为0，好处有：

- 极大地加速训练和推断
- 提高泛化性



因为L0范数不可微，所以不能直接加到目标函数上



Learning Sparse Neural Networks through L0 regularization

[code](https://github.com/AMLab-Amsterdam/L0_regularization)



[DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures](https://openreview.net/pdf?id=rylBK34FDS)

[code](https://github.com/yanghr/DeepHoyer)

## L1和L2

> 范数定义这里不再讲解，只给出一个图

![img](https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/Manhattan_distance.svg/200px-Manhattan_distance.svg.png)

上图出自Wiki-Taxicab geometry，绿线就是两点之间的L2距离，红线蓝线黄线都是L1距离



L1 和 L2 范数在机器学习上主要应用于：

- 用作损失函数
- 用作正则化项



## 损失函数

回归问题中，我们需要在一组数据点中找到一条线，使得点到线的总距离最小



1. 使用距离的L1范数作为损失函数，又被称为 **least absolute deviation (LAD，最小绝对偏差)**

$$
J=\sum_{i=1}^{n}\left|y_{i}-f\left(x_{i}\right)\right|
$$

2. 使用距离的L2范数作为损失函数，又被称为 **least squares error (LSE，最小二乘误差)**:

$$
J=\sum_{i=1}^{n}\left(y_{i}-f\left(x_{i}\right)\right)^{2}
$$

> 问题1：为什么大多使用L2范数作为损失函数呢？

求解的时候L2损失函数可以**直接求导，计算更方便**



L1损失函数也是有优点的，它**鲁棒性更强，对异常值更不敏感**，L2对大数的惩罚更大，



## 正则化项

正则化主要用于解决拟合问题中的**过拟合**，降低模型的复杂度，减少特征的数量

衡量一个模型复杂度可以用VC维，而一般VC维和系数 $w$ 的数量成正比；于是要降低模型的复杂度，减少系数 $w$ 的数量就行了，即限制 $w$ 中非零元素的数量。我们可以用一个优化问题来描述：
$$
\begin{aligned}
&\min _{w} J(w ; X, y) \\
&\text {s.t. }\|w\|_{0} \leq C
\end{aligned}
$$
这里用的L0范数，但L0范数不易求解，于是使用L1和L2范数来近似求解，

$$
\begin{aligned}
&\min _{w} J(w ; X, y) \\
&\text {s.t. }\|w\|_{1} \leq C
\end{aligned}
$$

$$
\begin{aligned}
&\min _{w} J(w ; X, y) \\
&\text {s.t. }\|w\|_{2} \leq C
\end{aligned}
$$

这样我们就可以使用拉格朗日算子，就上述带约束条件的优化问题写成拉格朗日方程
$$
L(w, \alpha)=J(w ; X, y)+\alpha\left(\|w\|_{1}-C\right)
$$
我们假设 $\alpha$ 的最优解为 $\alpha^*$ ，则上式等价于
$$
\min _{w} J(w ; X, y)+\alpha^{*}\|w\|_{1}
$$
这样其实正则化就是在原优化目标函数中增加了一个约束条件



正则化的标准表示形式如下：

L1-regularization 
$$
\mathbf{w}^{*}=\arg \min _{\mathbf{w}} \sum_{j}\left(y_j-\sum_{i} w_{i} h_{i}\left(\mathbf{x}_{j}\right)\right)^{2}
+\lambda \sum_{i=1}^{k}\left|w_{i}\right|
$$
L2-regularization
$$
\mathbf{w}^{*}=\arg \min _{\mathbf{w}} \sum_{j}\left(y_j-\sum_{i} w_{i} h_{i}\left(\mathbf{x}_{j}\right)\right)^{2}
+\lambda \sum_{i=1}^{k}w_{i}^2
$$






两者特点总总结：

- L2 计算起来更方便， L1 在非稀疏向量上的计算效率很低
- L1 最重要的一个特点，输出稀疏，会把不重要的特征直接置零，而 L2 则不会；稀疏性在面对large-scale的问题来说很重要，因为可以减少存储空间；实际上L1也是一种妥协的做法，要获得真正sparse的模型，要用L0正则化
- L2 有唯一解，而 L1 没有唯一解



再多探讨一下L1和L2正则项的梯度差异：

L1的梯度是两个相反的固定值（例如1和-1），每次更新迭代的时候，它都是稳步向0前进。

L2的梯度越靠近0，会越来越小。



L2 focuses more on large weights

L1 treats all weights equally.

英语表述：

L1 norm isn’t sensitive to the outlier

 

L1

<img src="C:\Users\Jerry\AppData\Roaming\Typora\typora-user-images\image-20200312195957213.png" alt="image-20200312195957213" style="zoom: 50%;" /> 

<img src="C:\Users\Jerry\AppData\Roaming\Typora\typora-user-images\image-20200312200332412.png" alt="image-20200312200332412" style="zoom: 50%;" /> 



https://zhuanlan.zhihu.com/p/29360425

https://zhuanlan.zhihu.com/p/44899616



![img](https://raw.githubusercontent.com/yzy1996/Image-Hosting/master/20200515200555)

这个图可以看出，0范数是非凸的





![img](https://upload.wikimedia.org/wikipedia/commons/6/60/Vector_norms.png)





## Tikhonov Regularization

适定性问题 (well-posed problem)，规定满足：

- 存在解
- 解是唯一的
- 解随着起始条件连续的改变



吉洪诺夫正则化 是为了解决非适定问题 (ill-posed problem) 的

通常和脊回归一起，只不过，吉洪诺夫拥有更大的集合



所谓吉洪诺夫正则化就是对权重的l2-norm的惩罚项。




$$
\min _{x}|A x-b|_{2}^{2}+|\Gamma x|_{2}^{2},
$$
where $\Gamma \in \mathbb{R}^{l \times n}$ is a weighting matrix or called Tikhonov matrix.





