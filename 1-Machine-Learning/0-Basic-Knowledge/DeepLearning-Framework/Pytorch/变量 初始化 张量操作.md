#### 初始化

```python
x = torch.randn(4, 4) # 正态分布
x = torch.rand(4, 4) # 均匀分布
x = torch.zeros(4, 4)
x = torch.ones(4, 4)
x = torch.tensor(4)
x = torch.arange(4)
x = torch.linspace(-10, 10, steps=5)

x = torch.zeros(5, 5).uniform_(-0.5, 0.5) # 得到的其实是一个均匀分布
```



### 查看size

```python
xx.size()
```



### 尺寸变换

```python
xx.view()
xx.reshape()

# view + contigious == reshape
```



### 维度变换

```python
# 数据不变，增加一维

# 交换维度
x = torch.randn(2, 3, 5)
x.size()
>>> torch.Size([2, 3, 5])
x.permute(2, 0, 1).size()
>>> torch.Size([5, 2, 3])

# 去掉“维数为1”的的维度
x.squeeze() # 括号内也可以跟某一维度

# 增加“维数为1”的的维度
x.unsqueeze(-1)
```



### Embed 初始化

```python
embd = nn.Embedding(5, 10)

# 访问
idx = torch.tensor([1, 2])
embd(idx)

# 查看权重，也就是具体数值
embd.weight.detach().numpy()

# 修改初始化权重
embd1.weight.data.normal_(1, 1) # 由N(0,1)改为N(1, 1)
```



类似于 list.append() 的拼接操作，逐步增加，

```python
a = [] 
for _ in range(2):
	b = ..
	b.unsqueeze_(1)
	a.append(b)
```





one layer with input size:
$$
(N, C, H, W)
$$
where N is a batch size, C denotes a number of channels, H is a height of input planes in pixels, and W is width in pixels.



kernel的组成

