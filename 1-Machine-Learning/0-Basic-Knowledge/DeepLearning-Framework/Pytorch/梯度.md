# 梯度

> 看知乎

PyTorch提供两种实现方法：`backward` and  `autograd.grad`



但不管是哪一种，首先我们都需要给想要求梯度的变量加上 `require`

`Tensor`变量有一个属性 `.requires_grad`，所以要想保留梯度需要额外的指示





### `backward`





### zero_grad()

通常会有 `model.zero_grad()` 和 `optimizer.zero_grad()` 两种

这两种都是为了把模型中参数的梯度设为0

当 `optimizer = optim.Optimizer(net.parameters()) `时，两者是等效的



### detach()









---

在pytorch的计算图里只有两种元素：数据（tensor）和 运算（operation）

数据可分为：









非叶子节点的tensor，如果想要获取他们的梯度，需要使用 `retain_grad()`





loss.backward() will compute the gradient

optimizer.step() will apply the gradient and update the tensor.

```python
loss1.backward(retain_graph=True)
loss2.backward(retain_graph=False)
optimizer.step()
optimizer2.step()
```





modified by an inplace operation



使用 **anomaly_detection**

```python
import torch


with torch.autograd.set_detect_anomaly(True):
    a = torch.rand(1, requires_grad=True)
    c = torch.rand(1, requires_grad=True)

    b = a ** 2 * c ** 2
    b += 1
    b *= c + a

    d = b.exp_()
    d *= 5

    b.backward()
    
sys:1: RuntimeWarning: Traceback of forward call that caused the error:
  File "tst.py", line 13, in <module>
    d = b.exp_()

Traceback (most recent call last):
  File "tst.py", line 16, in <module>
    b.backward()
  File "/Users/fmassa/anaconda3/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/Users/fmassa/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation
```











```python
loss.backward() 会计算当前参数 p0 经过前向过程 a0 后的各 tensor 的梯度
optimizer.step() 会更新参数 p0 -> p1

如果再次调用 loss.backward()
但loss 是在前p0基础上计算的，此刻又会要求在p1基础上计算，就产生了问题

you could either recalculate the forward pass to create the forward activations using the already updated parameters or update the parameters after all gradients were computed.
```

