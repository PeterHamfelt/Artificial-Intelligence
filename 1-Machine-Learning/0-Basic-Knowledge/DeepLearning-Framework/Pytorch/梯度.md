# 梯度

> 看知乎

PyTorch提供两种实现方法：`backward` and  `autograd.grad`



但不管是哪一种，首先我们都需要给想要求梯度的变量加上 `require`

`Tensor`变量有一个属性 `.requires_grad`，所以要想保留梯度需要额外的指示





### `backward`





### zero_grad()

通常会有 `model.zero_grad()` 和 `optimizer.zero_grad()` 两种

这两种都是为了把模型中参数的梯度设为0

当 `optimizer = optim.Optimizer(net.parameters()) `时，两者是等效的



### detach()









---

在pytorch的计算图里只有两种元素：数据（tensor）和 运算（operation）

数据可分为：